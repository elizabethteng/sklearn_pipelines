{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd0RI_IIgP0p"
   },
   "source": [
    "# Streamlining your machine learning workflows with Scikit-Learn pipelines\n",
    "\n",
    "This notebook introduces Scikit-Learn pipelines, which can help you organize and streamline your data processing and model training, as well as make your code cleaner and easier to manage.\n",
    "\n",
    "We will cover what pipelines are, why to use them in your machine learning code, review the components of `sklearn`'s [Pipeline](https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html), and go through an end-to-end example using a classification task, i.e. predicting a categorical outcome.\n",
    "\n",
    "## Section 0. Importing libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pcJPOkfgMBl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUajlt5OAmlp"
   },
   "source": [
    "We will be using a few datasets available through scikit-learn as examples in this workshop: MNIST, Wine, and Iris. All three of these sets are commonly used to demonstrate classification methods. Don't worry if the code used in this section is unfamiliar to you; it's just there to show you what the data look like.\n",
    "\n",
    "First, the classic MNIST dataset contains 28x28 pixel images of handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33cONOSLmZca"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset:\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist_X, mnist_y = mnist.data, mnist.target.astype(int)  # Features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4CzAIbePQ-R"
   },
   "source": [
    "**If the previous cell does not run successfully**, then you can manually upload the `mnist.npz` data file available on GitHub.\n",
    "\n",
    "**If you are using Google Colab**: un-comment out and run the following cell. Use the \"Choose Files\" button to upload the data file from your computer, then un-comment out and run the cell containing the `np.load` command.\n",
    "\n",
    "**If you are on a Jupyter host**: make sure the data file is saved to the same location as this notebook, then un-comment out and run only the cell containing the `np.load` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "RDM_Sw-t4NjQ",
    "outputId": "20f6666c-548b-454a-9c49-9dc264a67db7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# mnist = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lr6OTb9PJhN"
   },
   "outputs": [],
   "source": [
    "# data = np.load(\"mnist.npz\")\n",
    "# mnist_X, mnist_y = data['mnist_X'], data['mnist_y']\n",
    "# mnist_X = mnist_X.reshape(70000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4mpL1WvQzAT"
   },
   "source": [
    "Now that we have the MNIST dataset loaded up, we can visualize some of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "ENlEWu9fPBFl",
    "outputId": "48af4228-0647-4cdc-c9cd-3c44b09537f6"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of MNIST dataset images: {mnist_X.shape}\")\n",
    "print(f\"Shape of MNIST dataset target values: {mnist_y.shape}\")\n",
    "print(f\"MNIST dataset target values: {mnist_y}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(mnist_X[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Label: {mnist_y[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIwbT1TqAmr9"
   },
   "source": [
    "The 'Wine' dataset contains measurements of 178 wine samples for 13 chemical features including alcohol content, malic acid, total phenols, along with each sample's wine class (Class 0, 1, or 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "vdTAPy1sA9D8",
    "outputId": "1dce68e8-34cc-4a70-962f-102b26c79284"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "wine_X, wine_y = wine.data, wine.target\n",
    "\n",
    "print(f\"Shape of Wine dataset features: {wine_X.shape}\")\n",
    "print(f\"Shape of Wine dataset target values: {wine_y.shape}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for target_class in set(wine_y):\n",
    "    plt.scatter(\n",
    "        wine_X[wine_y == target_class, wine.feature_names.index('alcohol')],\n",
    "        wine_X[wine_y == target_class, wine.feature_names.index('color_intensity')],\n",
    "        label=f'Class {target_class}',\n",
    "        alpha=0.7\n",
    "    )\n",
    "plt.title('Wine Dataset: Alcohol vs. Color Intensity')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.legend(title='Wine Class')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-jyg3SvN0GWc",
    "outputId": "2b65fd7c-4cf5-4162-d3ed-b90efea12646"
   },
   "outputs": [],
   "source": [
    "# Convert dataset to a DataFrame for better readability\n",
    "wine_df = pd.DataFrame(wine_X, columns=wine.feature_names)\n",
    "\n",
    "# Set wine class\n",
    "wine_df['target'] = wine_y\n",
    "display(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kVLhukcB0WW"
   },
   "source": [
    "The 'Iris' dataset contains measurements of 150 iris flowers from three different species (Setosa, Versicolor, Virginica). Each flower sample in the dataset has 4 features: sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "kr40ffjiB1WM",
    "outputId": "f4593049-2db9-4f93-df00-e7d970d0322c"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris_X, iris_y = iris.data, iris.target\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for target_class in set(iris_y):\n",
    "    plt.scatter(\n",
    "        iris_X[iris_y == target_class, iris.feature_names.index('sepal length (cm)')],\n",
    "        iris_X[iris_y == target_class, iris.feature_names.index('sepal width (cm)')],\n",
    "        label=f'Species {target_class}',\n",
    "        alpha=0.7\n",
    "    )\n",
    "plt.title('Iris Dataset: Sepal Length vs. Sepal Width')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.legend(title='Iris Species')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u3FrezEiapf"
   },
   "source": [
    "## Section 1. Machine Learning workflows\n",
    "\n",
    "ML workflows often involve multiple steps: preprocessing data, feature engineering (i.e. selecting and transforming predictors), training models, and evaluating performance. Managing these workflows effectively can be tedious, especially as the complexity of datasets and models increases.\n",
    "\n",
    "Below is an example of a standard ML workflow for analyzing the MNIST dataset.\n",
    "\n",
    "First, we split the data to create separate training and testing datasets for training the model and evaluating model performance. The argument `test_size=0.2` specifies an 80/20 training/testing split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aHIkZDuLt6L",
    "outputId": "03a16372-b32c-4206-a302-4906e924ff9f"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mnist_X,\n",
    "                                                    mnist_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6TkgxAT7Nk-"
   },
   "source": [
    "### ML workflow step 1: Preprocessing the data\n",
    "\n",
    "Image data generally comes in a matrix of numbers, with each number signifying the brightness of one pixel in the image. When working with image data, the values can vary widely. Pixel values usually range from 0 to 255. To help machine learning models learn more effectively, we **standardize** these values using `StandardScaler` so that they have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c26JXuzX7Nug"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()                      # Initialize scaler object\n",
    "scaler.fit(X_train)                            # Fit scaler to training data\n",
    "X_train_scaled = scaler.transform(X_train)     # Scale training data\n",
    "X_test_scaled = scaler.transform(X_test)       # Scale test data using the same scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcpcsJuRM1ek"
   },
   "source": [
    "Exercise: What is the mean and standard deviation of the unscaled vs scaled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR31jelNM12x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcYPVFf37mai"
   },
   "source": [
    "### ML workflow step 2: Reducing dimensionality\n",
    "\n",
    "In machine learning, especially with image data or datasets with many features, we often deal with high-dimensional data. For example, a 28x28 pixel MNIST image has 784 features (one for each pixel). Processing all these features can slow down training, and increase the risk of overfitting noise. Using methods like Principal Component Analysis (PCA) can reduce the number of features while keeping the most important information. The argument `n_components=50` specifies that we want to reduce our dataset to 50 dimensions. The function `PCA.transform` converts data from the original 784 dimensions into the reduced 50 dimensions established by the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSlB_hwA7mgJ"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)                       # Initialize PCA object\n",
    "pca.fit(X_train_scaled)                          # Fit PCA to our scaled data\n",
    "X_train_reduced = pca.transform(X_train_scaled)  # Reduce dimensionality of training data\n",
    "X_test_reduced = pca.transform(X_test_scaled)    # Reduce dimensionality of testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLsSHGZbM7I9"
   },
   "source": [
    "Exercise: What are the dimensions of the data in the original vs reduced dimensions? Are these what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws4MUlNnM7N-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdfsIbz48Y9D"
   },
   "source": [
    "### ML workflow step 3: Train a logistic regression model for classification\n",
    "\n",
    "Here, the arguments `max_iter` and `solver` represent the maximum number of iterations in the training, and the algorithm used to optimize the model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "frM3wyZa8ZCQ",
    "outputId": "375a6447-4c67-48a7-f8c2-d8a57ae0035d"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "model.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K54beONUFjsb"
   },
   "source": [
    "### ML workflow step 4: Make predictions\n",
    "We can now use our model to make predictions based on the features of data that were not used in training (i.e., the testing set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cngfqSnFv8x"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoSAvb8FvyS"
   },
   "source": [
    "### ML workflow step 5: Evaluate model\n",
    "The function `classification_report` provides quantitative details on the accuracy of the model predictions, compared to the actual classes of the testing dataset.\n",
    "\n",
    "The metrics included are:\n",
    "1. Precision: percent of accurate predictions.\n",
    "2. Recall (Sensitivity): ability to identify all relevant cases.\n",
    "3. F1-Score: balance of precision and recall.\n",
    "4. Support: number of occurrances of each class in the set (to contextualize other metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-Kh7EKkibe5",
    "outputId": "30ddac46-7f1b-481e-c6b2-7c1b635958e4"
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-L-vYT_4V5r"
   },
   "source": [
    "### Exercise: write a workflow for either the Wine or Iris dataset, or a dataset of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiwJo_xU4WbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9sKIAtihyHB"
   },
   "source": [
    "## Section 2. Benefits of modularizing workflows with pipelines\n",
    "\n",
    "Machine learning workflows often involve multiple sequential steps, such as data preprocessing, dimensionality reduction, model training, and evaluation. Managing these steps individually can become cumbersome and error-prone, especially when conducting a series of similar analyses or experimenting with different methods.\n",
    "\n",
    "Pipelines in Scikit-Learn provide a solution by modularizing these workflows. (**Modularizing** means breaking a process down into smaller, self-contained steps or components, each responsible for a specific task, which can then be combined into a more complex process.) Instead of writing separate code blocks for each step, pipelines allow you to chain these steps into a single, cohesive object. This modular approach ensures that:\n",
    "\n",
    "* Steps are consolidated into a single object to reduce repetitive, generic code and to eliminate redundancy.\n",
    "* Steps are all executed in the correct order (e.g., scaling before PCA).\n",
    "* Preprocessing is consistently applied to both training and test data.\n",
    "* Code readability and maintainability are significantly improved.\n",
    "* Reproducibility is enhanced, as the entire workflow can be saved, shared, and reloaded.\n",
    "* Components can be swapped out easily to try other dimensionality reduction methods or predictive models without altering the rest of the pipeline.\n",
    "\n",
    "## Section 3. Understanding the Components of a Scikit-learn `Pipeline`\n",
    "\n",
    "A Scikit-Learn pipeline is composed of transformers and an estimator, combined into a cohesive workflow.\n",
    "\n",
    "We will take a look at some examples of transformers and estimators as applied to two scikit-learn datasets commonly used for classification: 'Wine' and 'Iris'.\n",
    "\n",
    "***Transformers*** are objects that preprocess data (e.g., scaling, encoding, or dimensionality reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dnT4ikdibcj"
   },
   "outputs": [],
   "source": [
    "# Transformer: StandardScaler to standardize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(wine_X)  # Calculate scaling so that mean=0 and st dev=1\n",
    "wine_X_scaled = scaler.transform(wine_X)  # Apply scaling to data\n",
    "\n",
    "# Transformer: PCA for dimensionality reduction\n",
    "pca = PCA(n_components=5) # Reduce features from 13 to 5\n",
    "pca.fit(wine_X_scaled)  # Learn principal components\n",
    "wine_X_reduced = pca.transform(wine_X_scaled)  # Reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZfS3dF46VKG"
   },
   "source": [
    "***Estimators*** are objects that train and predict (e.g., Logistic Regression, SVM, Random Forest). The examples given here can all be used as classifiers. In the code block below I show how each of these estimators are instantiated with relevant tunable hyperparameters. Each is then applied to the training data in the same way.\n",
    "\n",
    "It's ok if you're not familiar with these particular types of models; this is just to show that these estimators which all perform the same task are treated the same in scikit-learn syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXSqrEmV6YKH",
    "outputId": "31b6bfc7-20d4-4038-a4a1-23842fe57459"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split iris data into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, random_state=42)\n",
    "\n",
    "# Estimator: Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression Accuracy: {log_reg.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# Estimator: Support Vector Machine (SVM)\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')  # Radial Basis Function (RBF) kernel\n",
    "svm.fit(X_train, y_train)\n",
    "print(f\"SVM Accuracy: {svm.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# Estimator: Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "print(f\"Random Forest Accuracy: {random_forest.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# Estimator: k-Nearest Neighbors (k-NN)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"k-NN Accuracy: {knn.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aNI9BT_OFLV"
   },
   "source": [
    "### Question: what are some other examples of transformers and estimators that you can think of?\n",
    "\n",
    "Please type any examples you can think of in the chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQCfYddK9Nfb"
   },
   "source": [
    "## Section 4. Constructing a Scikit-Learn pipeline\n",
    "\n",
    "To create a `Pipeline` in scikit-learn, provide a list of steps where each step is a tuple consisting of:\n",
    "*   A name (a short label for the step)\n",
    "*   A transformer or estimator (the function that processes the data)\n",
    "\n",
    "The steps should follow the order of\n",
    "your workflow, with the final step being the model (estimator) that makes predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MILXgpht9Nt8"
   },
   "outputs": [],
   "source": [
    "# Instantiate sklearn Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MBXKMRO_bPJ"
   },
   "source": [
    "Once you have the pipeline instantiated, you can fit it to a dataset using `fit` and make predictions based on input data using `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0B_sq6V_fr3"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mnist_X[:100000],\n",
    "                                                    mnist_y[:100000],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KLg-CcLLz5g"
   },
   "source": [
    "Let's take a look at some of the predictions! Don't worry if the code used to make these visualizations is unfamiliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "ATIeBIdgMCFU",
    "outputId": "e8f28717-15a8-4efc-82eb-62fa859524e1"
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    # Select a random image from the test set\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "\n",
    "    # Reshape the flattened image back to 28x28\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "\n",
    "    # Plot the image\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show the predicted and true label\n",
    "    ax.set_title(f\"Pred: {y_pred[idx]}\\nTrue: {y_test[idx]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E1-JazuMB1s"
   },
   "source": [
    "We can also plot a **confusion matrix** to compare the true labels with the predicted labels to count correct and incorrect predictions, as well as which classes are more or less likely to be correctly predicted. Again, don't worry about the code used here to make the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "s-gx8e6WMFaA",
    "outputId": "ebfb587c-3414-46d4-cf81-53a15d98aaa6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix for MNIST Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICnwwuD-4WSc"
   },
   "source": [
    "## Section 5. Putting it all Together\n",
    "\n",
    "Use a pipeline to create, train, and evaluate a classifier for the Iris or Wine dataset, or a dataset of your own. You can use the same steps as in the first workflow introduced in Part 1, or any other combination of transformers and estimators given in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNQKiwa88AoY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ia6W9Jy4WZe"
   },
   "source": [
    "## Bonus: Create Your Own Pipeline!\n",
    "\n",
    "Try using a sklearn `Pipeline` to streamline an existing ML workflow from your own research!\n",
    "\n",
    "You can also try using a non-scikit-learn function in a pipeline, provided it implements the following methods:\n",
    "1. `fit(X,y)`: To train the model\n",
    "2. `predict(X)`: To make predictions\n",
    "3. `score(X,y)` for evaluation\n",
    "4. `transform(X)` for transformers\n",
    "\n",
    "Two examples of functions that meet these criteria are `XGBClassifier` from `XGBoost` and `LGBMClassifier` from `LightGBM`. You can also create your own custom functions that have the requisite methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMSwW6DO-ynE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
